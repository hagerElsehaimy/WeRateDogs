{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Wrangling and Analyzing - Udacity Data Analysis Nanodegree\n",
    "\n",
    "###### by Hager Mohamed\n",
    "\n",
    "## Table of Content\n",
    "\n",
    "- Problem Definition\n",
    "- Dataset\n",
    "- Data Gathering\n",
    "- Data Assessment\n",
    "    - visual assessment \n",
    "    - programatic assessment \n",
    "    - Quality issues\n",
    "    - Tidiness issues\n",
    "- Data cleanup\n",
    "- visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem Definition\n",
    "Real-world data rarely comes clean. Using Python and its libraries, you will gather data from a variety of sources and in a variety of formats, assess its quality and tidiness, then clean it. This is called data wrangling. You will document your wrangling efforts in a Jupyter Notebook, plus showcase them through analyses and visualizations using Python (and its libraries) and/or SQL.\n",
    "\n",
    "The dataset that you will be wrangling (and analyzing and visualizing) is the tweet archive of Twitter user @dog_rates, also known as WeRateDogs. WeRateDogs is a Twitter account that rates people's dogs with a humorous comment about the dog. These ratings almost always have a denominator of 10. The numerators, though? Almost always greater than 10. 11/10, 12/10, 13/10, etc. Why? Because \"they're good dogs Brent.\" WeRateDogs has over 4 million followers and has received international media coverage.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset\n",
    "#### Enhanced Twitter Archive\n",
    "\n",
    "The WeRateDogs Twitter archive contains basic tweet data for all 5000+ of their tweets, but not everything. One column the archive does contain though: each tweet's text, which I used to extract rating, dog name, and dog \"stage\" (i.e. doggo, floofer, pupper, and puppo) to make this Twitter archive \"enhanced.\" Of the 5000+ tweets, I have filtered for tweets with ratings only (there are 2356).\n",
    "\n",
    "#### Image Predictions File\n",
    "\n",
    "One more cool thing: I ran every image in the WeRateDogs Twitter archive through a neural network that can classify breeds of dogs*. The results: a table full of image predictions (the top three only) alongside each tweet ID, image URL, and the image number that corresponded to the most confident prediction (numbered 1 to 4 since tweets can have up to four images).\n",
    "\n",
    "<b>So for the last row in that table:</b>\n",
    "\n",
    "- tweet_id is the last part of the tweet URL after \"status/\" → https://twitter.com/dog_rates/status/889531135344209921\n",
    "- p1 is the algorithm's #1 prediction for the image in the tweet → golden retriever\n",
    "- p1_conf is how confident the algorithm is in its #1 prediction → 95%\n",
    "- p1_dog is whether or not the #1 prediction is a breed of dog → TRUE\n",
    "- p2 is the algorithm's second most likely prediction → Labrador retriever\n",
    "- p2_conf is how confident the algorithm is in its #2 prediction → 1%\n",
    "- p2_dog is whether or not the #2 prediction is a breed of dog → TRUE\n",
    "etc.\n",
    "\n",
    "#### Additional Data via the Twitter API\n",
    "\n",
    "Back to the basic-ness of Twitter archives: retweet count and favorite count are two of the notable column omissions. Fortunately, this additional data can be gathered by anyone from Twitter's API or download the json file uploaded in the nanodegree resources. \n",
    "\n",
    "********************"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Gathering\n",
    "1. Download archived file manually\n",
    "2. Download image prediction file by code through request package\n",
    "3. Download the twitter data programatically either through twitter API or through request from Udacity server "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests as req\n",
    "import os\n",
    "import json\n",
    "from bs4 import BeautifulSoup\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "function downlowd file from given url\n",
    "\n",
    "Input (str) represents url\n",
    "\n",
    "Returns Downloaded file name\n",
    "'''\n",
    "\n",
    "def request_file(url):\n",
    "    response = req.get(url)\n",
    "    file_name = url.split('/')[-1]\n",
    "    # save content of response in the file\n",
    "    if not os.path.isfile(file_name):\n",
    "        with open(file_name,'wb') as f:\n",
    "            f.write(response.content)\n",
    "    return file_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read archived tweets\n",
    "\n",
    "archive_df = pd.read_csv(\"twitter-archive-enhanced.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#download image prediction file\n",
    "\n",
    "url = \"https://d17h27t6h515a5.cloudfront.net/topher/2017/August/599fd2ad_image-predictions/image-predictions.tsv\"\n",
    "file_name = request_file(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read image prediction file\n",
    "\n",
    "prediction_df = pd.read_csv(file_name,sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_url = \"https://video.udacity-data.com/topher/2018/November/5be5fb7d_tweet-json/tweet-json.txt\"\n",
    "file_name = request_file(json_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import tweepy\n",
    "# from tweepy import OAuthHandler\n",
    "# from timeit import default_timer as timer\n",
    "\n",
    "#  Query Twitter API for each tweet in the Twitter archive and save JSON in a text file\n",
    "#  These are hidden to comply with Twitter's API terms and conditions\n",
    "# consumer_key = 'HIDDEN'\n",
    "# consumer_secret = 'HIDDEN'\n",
    "# access_token = 'HIDDEN'\n",
    "# access_secret = 'HIDDEN'\n",
    "\n",
    "# auth = OAuthHandler(consumer_key, consumer_secret)\n",
    "# auth.set_access_token(access_token, access_secret)\n",
    "\n",
    "# api = tweepy.API(auth, wait_on_rate_limit=True)\n",
    "\n",
    "#  NOTE TO STUDENT WITH MOBILE VERIFICATION ISSUES:\n",
    "#  df_1 is a DataFrame with the twitter_archive_enhanced.csv file. You may have to\n",
    "#  change line 17 to match the name of your DataFrame with twitter_archive_enhanced.csv\n",
    "#  NOTE TO REVIEWER: this student had mobile verification issues so the following\n",
    "#  Twitter API code was sent to this student from a Udacity instructor\n",
    "#  Tweet IDs for which to gather additional data via Twitter's API\n",
    "# tweet_ids = df_1.tweet_id.values\n",
    "# len(tweet_ids)\n",
    "\n",
    "#  Query Twitter's API for JSON data for each tweet ID in the Twitter archive\n",
    "# count = 0\n",
    "# fails_dict = {}\n",
    "# start = timer()\n",
    "#  Save each tweet's returned JSON as a new line in a .txt file\n",
    "# with open('tweet_json.txt', 'w') as outfile:\n",
    "#     # This loop will likely take 20-30 minutes to run because of Twitter's rate limit\n",
    "#     for tweet_id in tweet_ids:\n",
    "#         count += 1\n",
    "#         print(str(count) + \": \" + str(tweet_id))\n",
    "#         try:\n",
    "#             tweet = api.get_status(tweet_id, tweet_mode='extended')\n",
    "#             print(\"Success\")\n",
    "#             json.dump(tweet._json, outfile)\n",
    "#             outfile.write('\\n')\n",
    "#         except tweepy.TweepError as e:\n",
    "#             print(\"Fail\")\n",
    "#             fails_dict[tweet_id] = e\n",
    "#             pass\n",
    "# end = timer()\n",
    "# print(end - start)\n",
    "# print(fails_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create dataframe from json objects\n",
    "json_list=[]\n",
    "\n",
    "with open(file_name) as json_file:\n",
    "    for obj in json_file:\n",
    "        tweet = json.loads(obj)\n",
    "        tweet_id = tweet['id']\n",
    "        favorite_count = tweet['favorite_count']\n",
    "        retweet_count = tweet['retweet_count']\n",
    "        followers_count = tweet['user']['followers_count']\n",
    "        json_list.append({'tweet_id':tweet_id, \n",
    "                          'favorite_count':favorite_count,\n",
    "                          'retweet_count':retweet_count,\n",
    "                          'followers_count':followers_count})\n",
    "\n",
    "json_df = pd.DataFrame(json_list, columns= json_list[0].keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visual Assessment \n",
    "list the 3 dataframes for visual assessment "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "archive_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save json dataframe into csv format for visual assessment \n",
    "json_df.to_csv('twitter.csv',index=False)\n",
    "json_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Programmatic Assessment \n",
    "using pandas functions to assess the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "archive_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "archive_df.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "archive_df.tweet_id.isnull().any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "archive_df.tweet_id.duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "archive_df.name.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check rates validity\n",
    "archive_df.rating_numerator.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "archive_df.rating_numerator.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "archive_df.query('rating_numerator == 204')['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "archive_df.query('rating_numerator ==204')['text'][1120]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check denominatior consistency\n",
    "archive_df.rating_denominator.value_counts()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_df.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_df[prediction_df.jpg_url.duplicated() & prediction_df.img_num.duplicated()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rows that's not classified as dogs should be removed\n",
    "prediction_df.query('p1_dog == False & p2_dog == False & p3_dog == False')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_df[json_df.tweet_id.duplicated()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Quality Issues\n",
    "###### archive_df\n",
    "\n",
    "1. Drop rows indicates to be retweet or reply\n",
    "2. Drop data that doesn't have images in image prediction\n",
    "3. useless coloumns need to be dropped\n",
    "4. numerators and dominators datatypes need to be decimal\n",
    "5. values of numerators and dominators need to be fixed as mentioned in text cause it impacts quality of rates\n",
    "6. timestamp datatype should be converted into datetime object\n",
    "7. dog names in name coloumn have many typos \n",
    "8. some rows have None value which makes it meaningliess and should be dropped\n",
    "9. calculate rate from numerators and denominators\n",
    "10. Replace none value with NAN value\n",
    "###### prediction_df\n",
    "11. some image urls and num are duplicated which means repeatition in data\n",
    "12. some p#_dog are false as it's not even a dog\n",
    "\n",
    "###### json_df\n",
    "12. drop unused keys from json objects\n",
    "\n",
    "#### tidiness_issues\n",
    "1. archived_df 4 columns (dogger, floofer, pupper and puppo) for one variable (dog type)\n",
    "2. predictions: the dog breed prediction could be packed into one column (breed_pred)\n",
    "3. predictions: the prediction confidence could be packed into one column (pred_confidence)\n",
    "4. spread timestamp into 3 columns day, month and year\n",
    "5. merge all into one cleaned dataframe\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data CleanUp\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "backup data into backup dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# databackup\n",
    "\n",
    "archive_bkup = archive_df.copy()\n",
    "prediction_bkup = prediction_df.copy()\n",
    "tweetapi_bkup = json_df.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define\n",
    "1. drop rows that indecates to be retweet or reply"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "archive_df = archive_df[archive_df.in_reply_to_status_id.isnull()]\n",
    "archive_df = archive_df[archive_df.retweeted_status_id.isnull()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "archive_df.info()\n",
    "archive_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "******************\n",
    "\n",
    "#### Define\n",
    "\n",
    "2. fix datatype of timestamp coloumn to be datetime instead of string\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "archive_df['timestamp'] = pd.to_datetime(archive_df['timestamp'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "archive_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***********************\n",
    "#### Define\n",
    "\n",
    "3. split timestamp to 3 column"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "archive_df['year'] = archive_df.timestamp.dt.year\n",
    "archive_df['month'] = archive_df.timestamp.dt.month\n",
    "archive_df['day'] = archive_df.timestamp.dt.day"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "archive_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "****************************\n",
    "#### Define\n",
    "4. fix datatype of numerator and denominator to be decimal for accurate rating"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "archive_df['rating_numerator'] = archive_df['rating_numerator'].astype(float)\n",
    "archive_df['rating_denominator'] = archive_df['rating_denominator'].astype(float)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "archive_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*************\n",
    "#### Define\n",
    "5. calculate rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "archive_df['rate'] = (archive_df.rating_numerator / archive_df.rating_denominator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test\n",
    "archive_df.rate.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***********\n",
    "#### Define\n",
    "\n",
    "6. clean unlogical rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "archive_df.query('rate == 2.7')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "archive_df.query('rate == 2.7')['text'][763]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "archive_df.loc[(archive_df.tweet_id == 778027034220126208), 'rate'] = 1.127\n",
    "archive_df.rate.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "archive_df.query('rate == 42')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "archive_df.query('rate ==42')['text'][2074]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    archive_df.drop(index=2074,inplace=True)\n",
    "except:\n",
    "    pass\n",
    "archive_df.query('rate == 42')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "archive_df.query('rate ==0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "archive_df.query('rate ==0')['text'][315]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    archive_df.drop(index=315,inplace=True)\n",
    "except:\n",
    "    pass\n",
    "archive_df.query('rate == 0')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "archive_df.query('rate ==7.5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "archive_df.query('rate ==7.5')['text'][695]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "archive_df.loc[(archive_df.tweet_id == 786709082849828864), 'rate'] = 0.975\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "archive_df.query('rate == 2.6')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "archive_df.loc[(archive_df.tweet_id == 680494726643068929), 'rate'] = 1.126"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "archive_df.query('rate == 177.600000')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    archive_df.drop(979,inplace=True)\n",
    "except:\n",
    "    pass\n",
    "archive_df.query('rate == 177.600000')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    archive_df.drop(45,inplace=True)\n",
    "except:\n",
    "    pass\n",
    "archive_df.query('tweet_id == 883482846933004288')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "archive_df.rate.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "************\n",
    "#### Define\n",
    "8. concatenate dos type"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "archive_df.replace(to_replace='None', value='', inplace=True)\n",
    "archive_df['dogs_type'] = archive_df['doggo'] + archive_df['pupper'] + archive_df['puppo'] + archive_df['floofer']\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "archive_df.query('tweet_id == 854010172552949760')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define\n",
    "\n",
    "9. Drop useless columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    archive_df.drop(['in_reply_to_status_id',\n",
    "               'in_reply_to_user_id',\n",
    "               'timestamp',\n",
    "               'source',\n",
    "               'retweeted_status_id',\n",
    "               'retweeted_status_user_id',\n",
    "               'retweeted_status_timestamp',\n",
    "               'expanded_urls',\n",
    "               'rating_numerator',\n",
    "               'rating_denominator',\n",
    "                    'doggo',\n",
    "                    'floofer',\n",
    "                    'pupper',\n",
    "                    'puppo'],\n",
    "               axis=1,inplace=True)\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "archive_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "duplicated_index = prediction_df[prediction_df.jpg_url.duplicated() & prediction_df.img_num.duplicated()].index\n",
    "prediction_df.drop(duplicated_index,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "false_dogs = prediction_df.query('p1_dog == False & p2_dog == False & p3_dog == False').index\n",
    "prediction_df.drop(false_dogs,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "archive_df = pd.merge(archive_df,prediction_df,how='inner', on=\"tweet_id\")\n",
    "archive_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "archive_df['p_dog_max'] = archive_df['p1_dog']\n",
    "archive_df['p_max'] = archive_df['p1']\n",
    "for i, row in archive_df.iterrows():\n",
    "    archive_df['p_conf_max'] = archive_df[['p1_conf', 'p2_conf','p3_conf']].max(axis=1)\n",
    "    if row['p1_conf'] == archive_df['p_conf_max'][i]:\n",
    "        archive_df['p_dog_max'][i] = row['p1_dog']\n",
    "        archive_df['p_max'][i] = row['p1']\n",
    "    elif row['p2_conf'] == archive_df['p_conf_max'][i]:\n",
    "        archive_df['p_dog_max'][i] = row['p2_dog']\n",
    "        archive_df['p_max'][i] = row['p2']\n",
    "    elif row['p3_conf'] == archive_df['p_conf_max'][i]:\n",
    "        archive_df['p_dog_max'][i] = row['p3_dog']\n",
    "        archive_df['p_max'][i] = row['p3']\n",
    "    else:\n",
    "        archive_df['p_dog_max'][i] = np.nan\n",
    "        archive_df['p_max'][i] = np.nan\n",
    "archive_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    archive_df.drop(['p1',\n",
    "               'p1_conf',\n",
    "               'p1_dog',\n",
    "               'p2',\n",
    "               'p2_conf',\n",
    "               'p2_dog',\n",
    "               'p3',\n",
    "               'p3_conf',\n",
    "               'p3_dog'\n",
    "                    ],\n",
    "               axis=1,inplace=True)\n",
    "except:\n",
    "    pass\n",
    "archive_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "archive_df = pd.merge(archive_df,json_df,how='inner', on=\"tweet_id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "archive_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Store data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "archive_df.to_csv('twitter-cleaned.csv',index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,6))\n",
    "archive_df['tweet_id'].groupby([archive_df['year'], archive_df['month']]).count().plot(kind='line')\n",
    "plt.title('number of Tweets per month', size=20)\n",
    "plt.xlabel('Time (Year, Month)')\n",
    "plt.ylabel('Number of Tweets')\n",
    "\n",
    "fig = plt.gcf()\n",
    "fig.savefig('annual_tweets.png',bbox_inches='tight');\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,6))\n",
    "archive_df['followers_count'].groupby([archive_df['year'],archive_df['month']]).count().plot(kind='line')\n",
    "plt.title('number of followers per month', size=20)\n",
    "plt.xlabel('Time (Year, Month)')\n",
    "plt.ylabel('Number of followers')\n",
    "\n",
    "fig = plt.gcf()\n",
    "fig.savefig('annual_followers.png',bbox_inches='tight');\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,6))\n",
    "archive_df.plot(x='retweet_count', y='favorite_count', kind='scatter')\n",
    "plt.title('retweet & favourites correlation ', size=20)\n",
    "plt.xlabel('retweet_count')\n",
    "plt.ylabel('favorite_count')\n",
    "\n",
    "fig = plt.gcf()\n",
    "fig.savefig('retweet_favourite.png',bbox_inches='tight');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,6))\n",
    "archive_df.plot(x='retweet_count', y='followers_count', kind='scatter')\n",
    "plt.xlabel('tweet id')\n",
    "plt.ylabel('followers count')\n",
    "plt.title('tweet_id and followers correlation')\n",
    "\n",
    "fig = plt.gcf()\n",
    "fig.savefig('retweet_follower.png',bbox_inches='tight');\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,6))\n",
    "archive_df['rate'].value_counts().plot(kind='bar')\n",
    "plt.title ('Rating Distribution', size=20)\n",
    "plt.xlabel('Rate')\n",
    "plt.ylabel('Number of Ratings')\n",
    "\n",
    "fig = plt.gcf()\n",
    "fig.savefig('rating_distribution.png',bbox_inches='tight');\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,6))\n",
    "archive_df['dogs_type'].groupby(archive_df['rate']).value_counts().plot(kind='bar')\n",
    "plt.title('rate of dog type', size=20)\n",
    "plt.xlabel('dog type')\n",
    "plt.ylabel('rate')\n",
    "\n",
    "fig = plt.gcf()\n",
    "fig.savefig('dog_type_rate.png',bbox_inches='tight');\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dog_type = archive_df.query('dogs_type == \"doggo\"')\n",
    "plt.figure(figsize=(10,6))\n",
    "dog_type['rate'].groupby([dog_type['year'],dog_type['month']]).count().plot(kind='line')\n",
    "plt.title('rate of doggo type over the years', size=20)\n",
    "plt.xlabel('Time (Year, Month)')\n",
    "plt.ylabel('rate')\n",
    "\n",
    "fig = plt.gcf()\n",
    "fig.savefig('doggo_type_rate.png',bbox_inches='tight');\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dog_type = archive_df.query('dogs_type == \"pupper\"')\n",
    "plt.figure(figsize=(10,6))\n",
    "dog_type['rate'].groupby([dog_type['year'],dog_type['month']]).count().plot(kind='line')\n",
    "plt.title('rate of pupper type over the years', size=20)\n",
    "plt.xlabel('Time (Year, Month)')\n",
    "plt.ylabel('rate')\n",
    "\n",
    "fig = plt.gcf()\n",
    "fig.savefig('pupper_type_rate.png',bbox_inches='tight');\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dog_type = archive_df.query('dogs_type == \"floofer\"')\n",
    "plt.figure(figsize=(10,6))\n",
    "dog_type['rate'].groupby([dog_type['year'],dog_type['month']]).count().plot(kind='line')\n",
    "plt.title('rate of floofer type over the years', size=20)\n",
    "plt.xlabel('Time (Year, Month)')\n",
    "plt.ylabel('rate')\n",
    "\n",
    "fig = plt.gcf()\n",
    "fig.savefig('floofer_type_rate.png',bbox_inches='tight');\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dog_type = archive_df.query('dogs_type == \"puppo\"')\n",
    "plt.figure(figsize=(10,6))\n",
    "dog_type['rate'].groupby([dog_type['year'],dog_type['month']]).count().plot(kind='line')\n",
    "plt.title('rate of puppo type over the years', size=20)\n",
    "plt.xlabel('Time (Year, Month)')\n",
    "plt.ylabel('rate')\n",
    "\n",
    "fig = plt.gcf()\n",
    "fig.savefig('puppo_type_rate.png',bbox_inches='tight');\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
