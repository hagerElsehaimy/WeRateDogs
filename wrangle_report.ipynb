{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Wrangling and Analyzing - Udacity Data Analysis Nanodegree\n",
    "\n",
    "###### by Hager Mohamed\n",
    "\n",
    "## Table of Content\n",
    "\n",
    "- Problem Definition\n",
    "- Dataset\n",
    "- Data Gathering\n",
    "- Data Assessment\n",
    "    - visual assessment \n",
    "    - programatic assessment \n",
    "    - Quality issues\n",
    "    - Tidiness issues\n",
    "- Data cleanup\n",
    "- visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem Definition\n",
    "Real-world data rarely comes clean. Using Python and its libraries, you will gather data from a variety of sources and in a variety of formats, assess its quality and tidiness, then clean it. This is called data wrangling. You will document your wrangling efforts in a Jupyter Notebook, plus showcase them through analyses and visualizations using Python (and its libraries) and/or SQL.\n",
    "\n",
    "The dataset that you will be wrangling (and analyzing and visualizing) is the tweet archive of Twitter user @dog_rates, also known as WeRateDogs. WeRateDogs is a Twitter account that rates people's dogs with a humorous comment about the dog. These ratings almost always have a denominator of 10. The numerators, though? Almost always greater than 10. 11/10, 12/10, 13/10, etc. Why? Because \"they're good dogs Brent.\" WeRateDogs has over 4 million followers and has received international media coverage.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset\n",
    "#### Enhanced Twitter Archive\n",
    "\n",
    "The WeRateDogs Twitter archive contains basic tweet data for all 5000+ of their tweets, but not everything. One column the archive does contain though: each tweet's text, which I used to extract rating, dog name, and dog \"stage\" (i.e. doggo, floofer, pupper, and puppo) to make this Twitter archive \"enhanced.\" Of the 5000+ tweets, I have filtered for tweets with ratings only (there are 2356).\n",
    "\n",
    "#### Image Predictions File\n",
    "\n",
    "One more cool thing: I ran every image in the WeRateDogs Twitter archive through a neural network that can classify breeds of dogs*. The results: a table full of image predictions (the top three only) alongside each tweet ID, image URL, and the image number that corresponded to the most confident prediction (numbered 1 to 4 since tweets can have up to four images).\n",
    "\n",
    "<b>So for the last row in that table:</b>\n",
    "\n",
    "- tweet_id is the last part of the tweet URL after \"status/\" → https://twitter.com/dog_rates/status/889531135344209921\n",
    "- p1 is the algorithm's #1 prediction for the image in the tweet → golden retriever\n",
    "- p1_conf is how confident the algorithm is in its #1 prediction → 95%\n",
    "- p1_dog is whether or not the #1 prediction is a breed of dog → TRUE\n",
    "- p2 is the algorithm's second most likely prediction → Labrador retriever\n",
    "- p2_conf is how confident the algorithm is in its #2 prediction → 1%\n",
    "- p2_dog is whether or not the #2 prediction is a breed of dog → TRUE\n",
    "etc.\n",
    "\n",
    "#### Additional Data via the Twitter API\n",
    "\n",
    "Back to the basic-ness of Twitter archives: retweet count and favorite count are two of the notable column omissions. Fortunately, this additional data can be gathered by anyone from Twitter's API or download the json file uploaded in the nanodegree resources. \n",
    "\n",
    "********************"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Gathering\n",
    "1. Download archived file manually\n",
    "2. Download image prediction file by code through request package\n",
    "3. Download the twitter data programatically either through twitter API or through request from Udacity server \n",
    "*************************"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Assessment\n",
    "\n",
    "Data assessment has been implemented over 2 phases:\n",
    "\n",
    "1. visual assessment\n",
    "This has been done by the following:\n",
    "\n",
    "    - some simple methods on pandas dataframe like (sample, head, tail, info, describe)\n",
    "    - inspecting the csv files, json objects retrieved from twitter & some tweets through the expanded url\n",
    "    \n",
    "    \n",
    "2. programmatic assessment \n",
    "    - Programmatic assessment: pandas' functions and/or methods are used to assess the data(max,min,count,...etc).\n",
    "    \n",
    "At least eight (8) data quality issues and two (2) tidiness issues are detected, and include the issues to clean to satisfy the Project Motivation. Each issue is documented in one to a few sentences each.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Quality Issues\n",
    "###### archive_df\n",
    "\n",
    "1. Drop rows indicates to be retweet or reply\n",
    "2. Drop data that doesn't have images in image prediction\n",
    "3. useless coloumns need to be dropped\n",
    "4. numerators and dominators datatypes need to be decimal\n",
    "5. values of numerators and dominators need to be fixed as mentioned in text cause it impacts quality of rates\n",
    "6. timestamp datatype should be converted into datetime object\n",
    "7. dog names in name coloumn have many typos \n",
    "8. some rows have None value which makes it meaningliess and should be dropped\n",
    "9. calculate rate from numerators and denominators\n",
    "10. Replace none value with NAN value\n",
    "###### prediction_df\n",
    "11. some image urls and num are duplicated which means repeatition in data\n",
    "12. some p#_dog are false as it's not even a dog\n",
    "\n",
    "###### json_df\n",
    "12. drop unused keys from json objects\n",
    "\n",
    "#### tidiness_issues\n",
    "1. archived_df 4 columns (dogger, floofer, pupper and puppo) for one variable (dog type)\n",
    "2. predictions: the dog breed prediction could be packed into one column (breed_pred)\n",
    "3. predictions: the prediction confidence could be packed into one column (pred_confidence)\n",
    "4. spread timestamp into 3 columns day, month and year\n",
    "5. merge all into one cleaned dataframe\n",
    "\n",
    "************************************\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data cleanup\n",
    "\n",
    "1. Copies of the original pieces of data are made prior to cleaning.\n",
    "2. All issues identified in the assessment phase are successfully cleaned using Python and pandas.\n",
    "3. merge cleaned pieces to be in one dataframe\n",
    "********************************"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Store Data\n",
    "\n",
    "1. Save merged data in one CSV as cleaned version to be used in visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Visualisation \n",
    "\n",
    "some meaningful insights will be delivered to the end user and clarified in the visualization report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
